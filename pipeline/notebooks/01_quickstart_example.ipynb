{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arthropod Classification Pipeline - Quick Start Example\n",
    "\n",
    "This notebook demonstrates the complete workflow of the arthropod classification pipeline:\n",
    "\n",
    "1. **Image Processing**: Detection, segmentation, and extraction of specimens\n",
    "2. **Classification**: Hierarchical taxonomic classification\n",
    "3. **Export**: Results export to Excel and CSV\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- All dependencies installed (`pip install -r requirements.txt`)\n",
    "- YOLO models trained and available in `data/models/`\n",
    "- Sample composite images in `data/raw/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.config import Config\n",
    "from src.image_processing import SpecimenDetector, SpecimenSegmenter, SpecimenExtractor\n",
    "from src.classification import TaxonomyHierarchy, InferenceEngine\n",
    "from src.export import ExcelExporter, CSVExporter, StatisticsCalculator\n",
    "from src.utils.logging_config import setup_logging, get_logger\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_file=project_root / \"logs\" / \"notebook.log\")\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default configuration\n",
    "config_path = project_root / \"config\" / \"default_config.yaml\"\n",
    "config = Config(config_path=config_path)\n",
    "\n",
    "# Print key settings\n",
    "print(f\"Device: {config.get('device')}\")\n",
    "print(f\"Data root: {config.get('paths.data_root')}\")\n",
    "print(f\"Detection confidence: {config.get('image_processing.detection.confidence_threshold')}\")\n",
    "print(f\"Segmentation confidence: {config.get('image_processing.segmentation.confidence_threshold')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Processing\n",
    "\n",
    "Process a composite image to detect, segment, and extract individual specimens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "composite_image_path = project_root / \"data\" / \"raw\" / \"composite_S001_1mm.png\"\n",
    "sample_id = \"S001\"\n",
    "size_fraction = \"1\"\n",
    "output_dir = project_root / \"data\" / \"processed\" / sample_id\n",
    "\n",
    "print(f\"Processing: {composite_image_path}\")\n",
    "print(f\"Sample ID: {sample_id}\")\n",
    "print(f\"Size fraction: {size_fraction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "detector = SpecimenDetector(\n",
    "    size_fraction=size_fraction,\n",
    "    model_path=config.get('image_processing.detection.model_path'),\n",
    "    confidence_threshold=config.get('image_processing.detection.confidence_threshold'),\n",
    "    device=config.get('device')\n",
    ")\n",
    "\n",
    "# Detect specimens\n",
    "detections = detector.detect_specimens(composite_image_path)\n",
    "\n",
    "print(f\"✓ Detected {len(detections)} specimens\")\n",
    "print(f\"\\nFirst 3 detections:\")\n",
    "for i, det in enumerate(detections[:3]):\n",
    "    print(f\"  {i+1}. BBox: {det['bounding_box']}, Confidence: {det['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize segmenter\n",
    "segmenter = SpecimenSegmenter(\n",
    "    size_fraction=size_fraction,\n",
    "    model_path=config.get('image_processing.segmentation.model_path'),\n",
    "    confidence_threshold=config.get('image_processing.segmentation.confidence_threshold'),\n",
    "    dilation_factor=config.get('image_processing.segmentation.dilation_factor'),\n",
    "    device=config.get('device')\n",
    ")\n",
    "\n",
    "# Segment specimens\n",
    "segmentations = segmenter.segment_specimens(composite_image_path, detections)\n",
    "\n",
    "print(f\"✓ Segmented {len(segmentations)} specimens\")\n",
    "print(f\"\\nFirst 3 segmentations:\")\n",
    "for i, seg in enumerate(segmentations[:3]):\n",
    "    print(f\"  {i+1}. BBox: {seg['bounding_box']}, Confidence: {seg['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = SpecimenExtractor(output_dir=output_dir)\n",
    "\n",
    "# Extract specimens\n",
    "extraction_results = extractor.extract_specimens(\n",
    "    segmentations=segmentations,\n",
    "    sample_id=sample_id,\n",
    "    size_fraction=size_fraction\n",
    ")\n",
    "\n",
    "print(f\"✓ Extracted {len(extraction_results)} specimens to {output_dir}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = extractor.get_statistics(extraction_results)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Total specimens: {stats['total_specimens']}\")\n",
    "print(f\"  Avg confidence: {stats['avg_confidence']:.3f}\")\n",
    "print(f\"  Size range: {stats['min_size']}px - {stats['max_size']}px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize Sample Specimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 3 extracted specimens\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "specimen_files = sorted(output_dir.glob(\"*.png\"))[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(specimen_files), figsize=(12, 4))\n",
    "if len(specimen_files) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, img_path in zip(axes, specimen_files):\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(img_path.name)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "Classify extracted specimens using hierarchical taxonomic classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taxonomy hierarchy\n",
    "taxonomy_csv = project_root / \"data\" / \"taxonomy\" / \"catalogue_of_life.csv\"\n",
    "taxonomy = TaxonomyHierarchy(\n",
    "    csv_path=taxonomy_csv,\n",
    "    start_year=config.get('classification.start_year'),\n",
    "    end_year=config.get('classification.end_year')\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded taxonomy with {len(taxonomy.hierarchy)} taxa\")\n",
    "print(f\"  Root: {taxonomy.root_id}\")\n",
    "print(f\"  Major groups: {list(taxonomy.get_children(taxonomy.root_id))[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "model_dir = project_root / config.get('paths.models')\n",
    "inference_engine = InferenceEngine(\n",
    "    model_dir=model_dir,\n",
    "    model_set=config.get('classification.model_set'),\n",
    "    taxonomy=taxonomy,\n",
    "    default_threshold=config.get('classification.default_threshold'),\n",
    "    device=config.get('device')\n",
    ")\n",
    "\n",
    "# Classify all specimens in output directory\n",
    "specimen_images = list(output_dir.glob(\"*.png\"))\n",
    "print(f\"Classifying {len(specimen_images)} specimens...\")\n",
    "\n",
    "classification_results = inference_engine.classify_batch(\n",
    "    image_paths=specimen_images,\n",
    "    batch_size=config.get('classification.batch_size')\n",
    ")\n",
    "\n",
    "print(f\"✓ Classified {len(classification_results)} specimens\")\n",
    "print(f\"\\nFirst 3 classifications:\")\n",
    "for i, result in enumerate(classification_results[:3]):\n",
    "    print(f\"  {i+1}. {result['image_path'].name}:\")\n",
    "    print(f\"     Taxon: {result['predicted_taxon']}\")\n",
    "    print(f\"     Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"     Path: {' → '.join(result['path'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Results\n",
    "\n",
    "Export classification results to Excel and CSV formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive statistics\n",
    "stats_calculator = StatisticsCalculator()\n",
    "\n",
    "# Basic statistics\n",
    "basic_stats = stats_calculator.calculate_basic_stats(classification_results)\n",
    "print(\"Basic Statistics:\")\n",
    "print(f\"  Total specimens: {basic_stats['total_specimens']}\")\n",
    "print(f\"  Unique taxa: {basic_stats['unique_taxa']}\")\n",
    "print(f\"  Avg confidence: {basic_stats['avg_confidence']:.3f}\")\n",
    "\n",
    "# Per-taxon statistics\n",
    "per_taxon_stats = stats_calculator.calculate_per_taxon_stats(classification_results)\n",
    "print(f\"\\nTop 5 taxa by count:\")\n",
    "sorted_taxa = sorted(per_taxon_stats.items(), key=lambda x: x[1]['count'], reverse=True)[:5]\n",
    "for taxon, stats in sorted_taxa:\n",
    "    print(f\"  {taxon}: {stats['count']} specimens (avg conf: {stats['avg_confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Excel exporter\n",
    "excel_output_dir = project_root / \"output\" / \"excel\"\n",
    "excel_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "excel_exporter = ExcelExporter(output_dir=excel_output_dir)\n",
    "\n",
    "# Export results\n",
    "excel_path = excel_exporter.export(\n",
    "    results=classification_results,\n",
    "    filename=f\"{sample_id}_results.xlsx\",\n",
    "    statistics=basic_stats,\n",
    "    include_per_taxon=True,\n",
    "    include_hierarchy=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Excel exported to: {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CSV exporter\n",
    "csv_output_dir = project_root / \"output\" / \"csv\"\n",
    "csv_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_exporter = CSVExporter(output_dir=csv_output_dir)\n",
    "\n",
    "# Export results\n",
    "csv_path = csv_exporter.export(\n",
    "    results=classification_results,\n",
    "    filename=f\"{sample_id}_results.csv\",\n",
    "    include_path=True\n",
    ")\n",
    "\n",
    "# Export summary statistics\n",
    "summary_path = csv_exporter.export_summary(\n",
    "    summary=basic_stats,\n",
    "    filename=f\"{sample_id}_summary.csv\"\n",
    ")\n",
    "\n",
    "print(f\"✓ CSV exported to: {csv_path}\")\n",
    "print(f\"✓ Summary exported to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 View Results as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV results as pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Results DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Complete pipeline executed successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sample ID: {sample_id}\")\n",
    "print(f\"Size fraction: {size_fraction}\")\n",
    "print(f\"\")\n",
    "print(f\"Image Processing:\")\n",
    "print(f\"  Detections: {len(detections)}\")\n",
    "print(f\"  Segmentations: {len(segmentations)}\")\n",
    "print(f\"  Extracted: {len(extraction_results)}\")\n",
    "print(f\"\")\n",
    "print(f\"Classification:\")\n",
    "print(f\"  Total classified: {len(classification_results)}\")\n",
    "print(f\"  Unique taxa: {basic_stats['unique_taxa']}\")\n",
    "print(f\"  Avg confidence: {basic_stats['avg_confidence']:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Output:\")\n",
    "print(f\"  Specimens: {output_dir}\")\n",
    "print(f\"  Excel: {excel_path}\")\n",
    "print(f\"  CSV: {csv_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Process additional samples by changing `sample_id` and `size_fraction`\n",
    "- Process multiple samples using the batch script: `scripts/02_process_images.py`\n",
    "- Train your own models: `scripts/03_train_models.py`\n",
    "- Optimize thresholds for your data\n",
    "- Explore results in Excel or load CSVs in R/Python for further analysis\n",
    "\n",
    "For more information, see:\n",
    "- `README.md` - General overview\n",
    "- `docs/INSTALLATION.md` - Installation guide\n",
    "- `docs/ARCHITECTURE.md` - System architecture\n",
    "- Module-specific READMEs in `src/` subdirectories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
